{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on automatically computing Centrality\n",
    "\n",
    "This tutorial will teach you about computing centrality. Centrality determines the **importance of an proposition**.\n",
    "The centrality of a **proposition within an argument network** can be computed by **using different similarity measures**. **TextRank** is used to get the n most central propositions.\n",
    "\n",
    "This tutorial covers \n",
    "1. Centrality computed by using the Similarity Measure of *Mihalcea an Tarau (2004)*\n",
    "2. Centrality computed by doc2vec to increase performance.\n",
    "\n",
    "##### Disclaimer: \"This notebook is written  in favor of readability! It is NOT optimized towards efficency!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Get everything ready\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all libraries if necessary\n",
    "!pip install nltk\n",
    "!pip install networkx\n",
    "!pip install scipy\n",
    "!pip install gensim\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries used in this tutorial\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import string \n",
    "from gensim import utils\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nltk components if necessary\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the pre-traiend word2vec vectors from https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download and save them in __./data/GoogleNews-vectors-negative300.bin__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeding to ensure everybody gets the correct results\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Centrality computed by using the Similarity Measure of *Mihalcea an Tarau (2004)*\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured in the __Argument Interchange Format(AIF)__ format that you can see below. \n",
    "\n",
    "For our purpose only nodes of type \"I\" are important, as they contain propositional information for arguments such as premise or conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](aif.png)\n",
    "\n",
    "\n",
    "\n",
    "Taken from: LAWRENCE, John; REED, Chris. Using complex argumentative interactions to reconstruct the argumentative structure of large-scale debates. In: Proceedings of the 4th Workshop on Argument Mining. 2017. S. 108-117."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the nodesets\n",
    "# choose the I-nodes for computation and save the corresponding text\n",
    "directory = os.path.join(os.getcwd(),'US2016R1reddit')\n",
    "I_nodes = {}\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(directory,filename), 'r') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            for i in range(len(json_data['nodes'])):\n",
    "                if json_data['nodes'][i]['type'] == \"I\":\n",
    "                    nodeID = json_data['nodes'][i]['nodeID']  \n",
    "                    I_nodes[nodeID] = json_data['nodes'][i]['text']\n",
    "\n",
    "# create a pandas dataframe for the data\n",
    "df = pd.DataFrame.from_dict(I_nodes, orient ='index', columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the data\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing stage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we want to __tokenize__ all nodes, __remove all punctuation__ and perform __stemming__ for each token.\n",
    "\n",
    "Your tasks are:\n",
    "1. Remove the punctuation from all texts\n",
    "2. Tokenize the texts\n",
    "3. Perform stemming\n",
    "\n",
    "##### Example: \n",
    "they don't do terrorist attacks like they used to --> ['they', 'do', \"n't\", 'do', 'terrorist', 'attacks', 'like', 'they', 'used','to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_all = []\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Tokenization via NLTK implementation and removing punctations\n",
    "for row in df.iterrows():\n",
    "    text=row[1].text\n",
    "    \n",
    "    ###To do: Remove the punctuation from all texts####\n",
    "\n",
    "    #####################################\n",
    "    \n",
    "    ###To do: Tokenize the texts####\n",
    "\n",
    "    #####################################\n",
    "    \n",
    "    ###To do: Perform stemming####\n",
    "\n",
    "    #####################################\n",
    "        \n",
    "    tokens_all.append(tokens)\n",
    "\n",
    "# save to dataframe\n",
    "df['text_preprocessed'] = tokens_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the data\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us assure everybody got the same results\n",
    "#assert df['text_preprocessed'][0]  == ['they', 'do', \"n't\", 'do', 'terrorist', 'attack', 'like', 'they', 'use', 'to']\n",
    "#assert df['text_preprocessed'][10] == ['carson', 'want', 'a', 'flat', 'tax']\n",
    "#assert df['text_preprocessed'][50] == ['I', 'have', 'no', 'word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of TextRank\n",
    "##### What is TextRank and how does it work? \n",
    "TextRank is a graph-based raking model for natural language text processing.\n",
    "Graph-based ranking algorithms quantify the importance of vertices within a graph, based on global information extracted from the entire graph. For our application this basically means the corpus (the entirety of texts) is represented by a single graph. In this graph we are raking parts of this graph (the single texts) by their importance in the entire graph.\n",
    "To achieve this, for main steps have to be taken:\n",
    "> 1. Identify text units that best define the task at hand, and add them as vertices in the graph.\n",
    "> 2. Identify relations that connect such text units, and use these relations to draw edges between vertices in the graph. Edges can be directed or undirected, weighted or unweighted.\n",
    "> 3. Iterate the graph-based ranking algorithm until convergence.\n",
    "> 4. Sort vertices based on their final score. Use the values attached to each vertex for ranking/selection decisions.\n",
    "> \n",
    "> *Source: Mihalcea, R., & Tarau, P. (2004). Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing.*\n",
    "\n",
    "_Lucky for us, there is already an implementation out there._"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Computation done by similarity measure of Michalcea and Tarau (2004)__\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Text1:['carson', 'want', 'a', 'flat', 'tax']\n",
    "\n",
    "Text2:['carson', 'wants', 'two', 'flat', 'taxes']\n",
    "\n",
    "Similarity(Text1, Text2) = 2 / log(5)+log(5) = 0.6213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity measure by Michalcea and Tarau (2004)\n",
    "\n",
    "# Compute similarity matrix\n",
    "sim_dict = {}\n",
    "##### To do : implement the similarity measure #########\n",
    "\n",
    "#######################################################    \n",
    "\n",
    "# Print similarity matrix\n",
    "sim = pd.DataFrame(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the similarity matrix\n",
    "sim.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us assure everybody got the same results\n",
    "with open('similarity_assert.pkl', 'rb') as f: assert sim['173836'].tolist() == pickle.load(f).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply TextRank algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataframe into a n-dimensional Numpy-array\n",
    "sim_matrix = sim.values\n",
    "\n",
    "# TextRank\n",
    "nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# Sort TextRank Scores\n",
    "sorted_scores_Common_words = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best 5 central texts\n",
    "for i in range(5):\n",
    "    node_ID = sorted_scores_Common_words[i][0]\n",
    "    id_ = sim.index[node_ID]\n",
    "    print(df.text[df.index[df.index == id_][0]])\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Centrality computed by word2vec for increased performance.\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Word2Vec model\n",
    "\n",
    "##### What is doc2vec and how does it work?\n",
    "Word2vec represents the meaning of words and their relations in vector space, making every word become a feature vector. What this allows us to do is to derive related words from a specific word. A classic example you probably already know is the \"king\"-example. It basically tells that king is to queen what men is to women.\n",
    "\n",
    "![alt text](kingqueen.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors directly from the file\n",
    "model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we want to compare all texts using the cosine similarity measure. Since word2vec only gives us vectors for each word we have to create the document vectors on our own.\n",
    "\n",
    "__Your tasks are:__\n",
    "1. To compute the similarity between each pair of texts, we compute the centroids of each text. \n",
    "2. The centroid of a text is the average of all its token vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "##### To do : compute the centroids for each text #########\n",
    "\n",
    "#######################################################\n",
    "df[\"w2v_centorids\"] = centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the centroids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our document vecors, we can compute the cosine similarity of each document pair.\n",
    "\n",
    "__Your tasks are:__\n",
    "1. The centroids are then compared using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute consine similarity\n",
    "\n",
    "# Compute similarity matrix\n",
    "sim_dict = {}\n",
    "##### To do : implement the similarity measure #########\n",
    "\n",
    "#######################################################    \n",
    "\n",
    "# Print similarity matrix\n",
    "sim = pd.DataFrame(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the similarity matrix\n",
    "sim.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan values with 0\n",
    "sim = sim.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us assure everybody got the same results\n",
    "with open('w2v_assert.pkl', 'rb') as g: assert sim['173836'].tolist() == pickle.load(g).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply TextRank algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataframe into a n-dimensional Numpy-array\n",
    "sim_matrix = sim.values\n",
    "\n",
    "# TextRank\n",
    "nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# Sort TextRank Scores\n",
    "sorted_scores_word2vec = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best 5 central texts\n",
    "for i in range(5):\n",
    "    node_ID = sorted_scores_word2vec[i][0]\n",
    "    id_ = sim.index[node_ID]\n",
    "    print(df.text[df.index[df.index == id_][0]])\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating the computed centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will:\n",
    "1. Computing the centrality based on the graph structure <br>\n",
    "2. Computing Kendalls rank correlation <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the ground truth results based on the centrality measure represented in the lecture, we need to recreate the original graph from the data. However for our purpose only reasoning-nodes(RA) and conflict-nodes(CA) are of relevance.\n",
    "\n",
    "__Hint:__ You can think of RA-nodes as support and CA-nodes as attacks towards an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the nodesets\n",
    "#choose nodes which are supporting or conflict nodes and safe the node where the edge comes from\n",
    "directory = os.path.join(os.getcwd(),'US2016R1reddit')\n",
    "interesting_nodes = {}\n",
    "nodeID = 0\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(directory,filename), 'r') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            for i in range(len(json_data['nodes'])):\n",
    "                edges = []\n",
    "                if json_data['nodes'][i]['type'] == \"CA\" or json_data['nodes'][i]['type'] == \"RA\" :\n",
    "                    nodeID = json_data['nodes'][i]['nodeID'] \n",
    "                for j in range (len(json_data['edges'])):\n",
    "                        if json_data['edges'][j]['toID'] == nodeID:\n",
    "                            edges.append(json_data['edges'][j]['fromID'])\n",
    "                            interesting_nodes [nodeID] = edges\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the graph, values are outgoing edges\n",
    "print(interesting_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we have the data prepared we can compute the centrality measurement for all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building up a graph from what we found out ready to do computation of centrality \n",
    "list_of_nodes = []\n",
    "list_of_edges = []\n",
    "for key, value in interesting_nodes.items():\n",
    "    list_of_nodes.append(key)\n",
    "    list_of_nodes.extend(value)\n",
    "    for i in range(len(value)):\n",
    "        edge = (value[i],key)\n",
    "        list_of_edges.append(edge)\n",
    "\n",
    "#actually build up the sub graph by using Networkx libary   \n",
    "sub_graph=nx.Graph()\n",
    "sub_graph.add_nodes_from(list_of_nodes)\n",
    "sub_graph.add_edges_from(list_of_edges)\n",
    "\n",
    "# compute centrality\n",
    "centrality = nx.eigenvector_centrality(sub_graph, max_iter=500, tol=1e-06, nstart=None, weight='weight')\n",
    "\n",
    "# delete support and attack nodes\n",
    "for key, value in interesting_nodes.items():\n",
    "    del centrality[key]\n",
    "\n",
    "sorted_scores_onGraph = sorted(centrality.items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next step is to map the ranking back to the original nodes in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the ranking back to the original instances in the df\n",
    "ranking = [i[0] for i in sorted_scores_onGraph]\n",
    "centrality_rankings = []\n",
    "for row in df.iterrows():\n",
    "    try:\n",
    "        centrality_rankings.append(ranking.index(row[0]))\n",
    "    except:\n",
    "        centrality_rankings.append(1173)\n",
    "        \n",
    "df['centrality_ranking'] = centrality_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the ranking back to the original instances in the df\n",
    "list_w2v = []\n",
    "for i in range(len(sorted_scores_word2vec)):\n",
    "    list_w2v.append(sim.index[sorted_scores_word2vec[i][0]])\n",
    "\n",
    "w2v_rankings = []\n",
    "for row in df.iterrows():\n",
    "    try:\n",
    "        w2v_rankings.append(list_w2v.index(row[0]))\n",
    "    except:\n",
    "        w2v_rankings.append(1173)\n",
    "        \n",
    "df['word2vec_ranking'] = w2v_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the ranking back to the original instances in the df\n",
    "list_cw = []\n",
    "for i in range(len(sorted_scores_Common_words)):\n",
    "    list_cw.append(sim.index[sorted_scores_Common_words[i][0]])\n",
    "    \n",
    "common_words_rankings = []\n",
    "for row in df.iterrows():\n",
    "    try:\n",
    "        common_words_rankings.append(list_cw.index(row[0]))\n",
    "    except:\n",
    "        common_words_rankings.append(1173)\n",
    "        \n",
    "df['common_words_ranking'] = common_words_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at our final dataframe\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After we've computed the ground truth centrality we can now compare our text based algorithms. \n",
    "To do this we use Kendalls rank correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute kendalls tau for rank correlation onGraph and ranking_Common_word\n",
    "kendalls_tau, p_value = stats.kendalltau(df['centrality_ranking'], df['common_words_ranking'])\n",
    "print(kendalls_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute kendalls tau for rank correlation onGraph and ranking_Common_word\n",
    "kendalls_tau, p_value = stats.kendalltau(df['centrality_ranking'], df['word2vec_ranking'])\n",
    "print(kendalls_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
